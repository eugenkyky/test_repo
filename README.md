HTTP API test assignment
=====================
## Installation

The most convenient way is to use Vagrant for deploy working enviroment. For this:
```bash
$ cd ~
$ mkdir test_assignment
$ cd test_assignment
$ wget 'https://raw.githubusercontent.com/eugenkyky/xsolla_test_repo/master/deploy.sh'
$ wget 'https://raw.githubusercontent.com/eugenkyky/xsolla_test_repo/master/Vagrantfile'
$ vagrant box add hashicorp/precise64
$ vagrant init
$ vagrant up
```

## Usage

After the whole environment was set (deploying code can be found in ```deploy.sh```) you can connect to the virtual machine and perform tests
```bash
$ vagrant ssh
$ cd test_assignment
$ vendor/bin/phpunit test/Services/..
```

You can delete users files by executing next:
```bash
$ sudo rm users_files/user/*
```

## Documentation

### HTTP API methods:
```
Ð¡reate file:  	HTTP POST /files/{filename}  
Update file:  	HTTP PUT  /files/{filename}
Download file:  HTTP GET  /files/{filename}
Get file meta:  HTTP GET  /files/{filename}/meta
Get file list:  HTTP GET  /files
```

### Additional features

#### Working with PHP ecosystem:
* composer for dependency managment
* silex framework and symfony components. 
* php7.0 features: scalar type declarations, return type declarations, group use declarations. 

#### Work structure 
* readme
* github

#### Optimization
Download and upload file optimization (first of all the memory use)
The best way for working with RAM memory will be implantation opportunity to work with chunks of files.
The bottom line is that just would not work with a whole file in the code to handle requests, but only whith part of file, determined by the byte order.

Thus we obtain the following advantages:

1. Renewable upload/download file
2. Smoothing memory consumption peaks
3. Work with very large files
4. Processing a particular file chunk

Approximate algorithm:
Downloading from server:
```
  Client:
  > GET /files/filename HTTP/1.1
  > Host: server
  > Range: bytes 21010-47021/47022
    
  Server:
  < HTTP/1.1 206 
  < Content-Range: bytes 21010-47021/47022
  < Content-Length: 26012
  < bytes 21010-47021
```
Uploading to server:
```
  Client:
  > POST /files/filename HTTP/1.1
  > Host: server
  > Content-Range: bytes 21010-47021/47022
  > Content-Length: 26012
  > bytes 21010-47021
    
  Server:
  < HTTP/1.1 100 
  < Content-Range: bytes 21010-47021/47022
  < Content-Length: 26012
  < bytes 21010-47021
```

Ability to work directly with a file should be left on the server for opportunity to work with clients who cannot work with chunks.


#### Gzip compressing response
Typically this feature is enabled by web-server. Will reduce the delivery time.

#### Gzip request support
This option enables by request url parameter ```file_encode``` and setiing value to ```gzip```. 

#### File storage in compressed form
I don't think that it is necessary to implement this mechanism for the following reasons:
1. In the case of transmission chunks of a file for each new request server needs to decompress the entire file that would read the part. Keeping in RAM unzipped file is unreasonable for consumption of RAM.
2. Long-term memory - not an expensive resource.

#### Serving very big files
See download and upload file optimization section

#### Concurrent access to file

In the case of work with the whole file realised functionality is enough. Function ```Filesystem->dumpFile()``` works atomically. File that the user read is always either old version or completely new version of the file. If server needs to work with parts of the file, we need to use a mutex, LockHandler function of Symfony framework to lock the file at the time of working with them.
Like that:

```php
use Symfony\Component\Filesystem\LockHandler;
$lockHandler = new LockHandler('hello.lock');
if (!$lockHandler->lock()) {
    // the resource "hello" is already locked by another process
    return Response('Oops');
} else {
	doWork();
}
```

#### Access control
In my work authenticated user has the ability to write only in single directory whose name matches the user name. Function getCurrentUserDir(Application $app): string returns the directory name for the current user.

#### Limits on the size of uploaded files

I think it makes no sense to do a limit on the size of uploaded files as user files can be huge and this can be critical for him. Correctly use the file upload by chunks and install the server limit on the size of the each request.


#### API authorization mechanism 
First, when user is authenticated he gets read/write access to the folder.

#### Authentication process
First, user register at service, next he is given the key - randomly generated bytes sequence  - which knows the server and the client and he need to keep it in secret.
Next, for each request, the system checks the availability of key, checks whether the key belongs to a user, if successful - user takes  wright/reading access to folder.
It will only work with https

#### Number of requests to the API for the client and/or ip address restrictions 
The correct one for this counter will using database in RAM to reduce time response, the easiest way to do it by REDIS as described here: http://redis.io/commands/incr

#### Attach file to creator and protect it from users changing
Implemented by the fact that the user is able to work only with its folder.

#### Place quotas and limit for each user
Each writing or updating file system check possible consumable place after write operation. Consumed place plus new bytes and compare with quota.

#### Environment 
Start from PHP built-in web server 
Tuned Vagrant 

#### Other
Right Content-Type in headers for each file
